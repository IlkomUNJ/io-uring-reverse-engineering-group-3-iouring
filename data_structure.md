| Structure name | Defined in | Attributes | Caller Functions Source | Source Caller | Usage |
|---|---|---|---|---|---|
| io_fadvise | advise.c | offset, len, advice | switch | list_for_each_entry_rcu, list_for_each_entry, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_fadvise {; static bool io_fadvise_force_async(struct io_fadvise *fa); int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe) |
| io_madvise | advise.c | addr, len, advice | - | - | struct io_madvise {; int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_madvise *ma = io_kiocb_to_cmd(req, struct io_madvise); |
| io_cancel | cancel.c | addr, flags, fd, opcode | list_for_each_entry, switch, __wq_list_for_each, hlist_for_each_entry_safe, while, hlist_for_each_entry | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_cancel {; bool io_cancel_req_match(struct io_kiocb *req, struct io_cancel_data *cd); if (io_cancel_match_sequence(req, cd->seq)) |
| io_cancel_data | cancel.h | data | list_for_each_entry, switch, hlist_for_each_entry_safe, while, hlist_for_each_entry | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | bool io_cancel_req_match(struct io_kiocb *req, struct io_cancel_data *cd); struct io_cancel_data *cd = data;; struct io_cancel_data *cd) |
| io_epoll | epoll.c | epfd, op, fd, event | - | - | struct io_epoll {; struct io_epoll_wait {; int io_epoll_ctl_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe) |
| io_epoll_wait | epoll.c | maxevents | - | - | struct io_epoll_wait {; int io_epoll_wait_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_epoll_wait *iew = io_kiocb_to_cmd(req, struct io_epoll_wait); |
| io_ev_fd | eventfd.c | eventfd_async, last_cq_tail, refs, ops, rcu | if | list_for_each_entry, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_ev_fd {; struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);; static void io_eventfd_put(struct io_ev_fd *ev_fd) |
| io_rename | fs.c | old_dfd, new_dfd, flags | - | - | struct io_rename {; int io_renameat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_rename *ren = io_kiocb_to_cmd(req, struct io_rename); |
| io_unlink | fs.c | dfd, flags | - | - | struct io_unlink {; int io_unlinkat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_unlink *un = io_kiocb_to_cmd(req, struct io_unlink); |
| io_mkdir | fs.c | dfd, mode | - | - | struct io_mkdir {; int io_mkdirat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_mkdir *mkd = io_kiocb_to_cmd(req, struct io_mkdir); |
| io_link | fs.c | old_dfd, new_dfd, flags | switch, if, while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_link {; struct io_link *sl = io_kiocb_to_cmd(req, struct io_link);; struct io_link *sl = io_kiocb_to_cmd(req, struct io_link); |
| io_futex | futex.c |  | switch, if, io_for_each_link, list_for_each_entry | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | ret = io_futex_cancel(ctx, cd, issue_flags);; struct io_futex {; struct io_futex_data { |
| io_futex_data | futex.c | q | - | - | struct io_futex_data {; sizeof(struct io_futex_data), 0);; struct io_futex_data *ifd = req->async_data; |
| io_worker | io-wq.c | ref, flags, nulls_node, all_list, lock, ref_done, create_state, create_work, init_retries, rcu, work | switch, hlist_nulls_for_each_entry_rcu, while, list_for_each_entry_rcu | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_worker {; static bool create_io_worker(struct io_wq *wq, struct io_wq_acct *acct);; static void io_wq_dec_running(struct io_worker *worker); |
| io_wq_acct | io-wq.c | workers_lock, nr_workers, max_workers, nr_running, free_list, all_list, lock, work_list, flags | switch, hlist_nulls_for_each_entry_rcu, while, list_for_each_entry_rcu | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_wq_acct *acct;; struct io_wq_acct {; struct io_wq_acct acct[IO_WQ_ACCT_NR]; |
| io_wq | io-wq.c | state, worker_refs, worker_done, cpuhp_node, wait, cpu_mask | list_for_each_entry_rcu, list_for_each_entry, switch, io_for_each_link, __wq_list_for_each, if, xa_for_each, while, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | static bool io_cancel_cb(struct io_wq_work *work, void *data); enum io_wq_cancel cancel_ret;; if (!tctx || !tctx->io_wq) |
| io_cb_cancel_data | io-wq.c | nr_running, nr_pending, cancel_all | switch, list_for_each_entry_rcu | list_for_each_entry_rcu, list_for_each_entry, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_cb_cancel_data {; struct io_cb_cancel_data *match);; struct io_cb_cancel_data match = { |
| online_data | io-wq.c | cpu, online | list_for_each_entry_rcu | list_for_each_entry, switch, scoped_guard, if, while, hlist_for_each_entry_rcu, wq_list_for_each, hlist_nulls_for_each_entry_rcu, for | struct online_data {; struct online_data *od = data;; struct online_data od = { |
| io_wq_hash | io-wq.h | refs, map, wait | io_for_each_link, list_for_each_entry_rcu | list_for_each_entry, switch, list_for_each_prev, scoped_guard, __wq_list_for_each, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_wq_hash *hash;; void io_wq_hash_work(struct io_wq_work *work, void *val); static int io_wq_hash_wake(struct wait_queue_entry *wait, unsigned mode, |
| io_wq_data | io-wq.h |  | list_for_each_entry_rcu | list_for_each_entry, switch, scoped_guard, if, while, hlist_for_each_entry_rcu, wq_list_for_each, hlist_nulls_for_each_entry_rcu, for | struct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data); struct io_wq_data {; struct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data); |
| io_defer_entry | io_uring.c | list, seq | io_for_each_link, list_for_each_entry_reverse, __wq_list_for_each | list_for_each_entry, switch, io_for_each_link, list_for_each_prev, __wq_list_for_each, list_for_each_entry_safe, if, xa_for_each, while, list_for_each_entry_reverse | struct io_defer_entry {; struct io_defer_entry *de = list_first_entry(&ctx->defer_list,; struct io_defer_entry, list); |
| ext_arg | io_uring.c | argsz, ts, min_time, ts_set, iowait | xa_for_each, __wq_list_for_each | list_for_each_entry, switch, io_for_each_link, __wq_list_for_each, if, xa_for_each, while, list_for_each_entry_reverse, for | struct ext_arg {; struct ext_arg *ext_arg,; if (ext_arg->iowait && current_pending_io()) |
| io_tctx_exit | io_uring.c | task_work, completion | __wq_list_for_each | list_for_each_entry, io_for_each_link, if, xa_for_each, while, list_for_each_entry_reverse | struct io_tctx_exit {; static __cold void io_tctx_exit_cb(struct callback_head *cb); struct io_tctx_exit *work; |
| io_task_cancel | io_uring.c | all | list_for_each_entry, __wq_list_for_each | list_for_each_entry_rcu, list_for_each_entry, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_task_cancel {; struct io_task_cancel *cancel = data;; struct io_task_cancel cancel = { .tctx = tctx, .all = cancel_all, }; |
| io_wait_queue | io_uring.h | wq, cq_tail, cq_min_tail, nr_timeouts, hit_timeout, min_timeout, timeout, t, napi_busy_poll_dt, napi_prefer_busy_poll | switch, hlist_for_each_entry_rcu, __wq_list_for_each, list_for_each_entry_rcu | list_for_each_entry_rcu, list_for_each_entry, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_wait_queue *iowq = container_of(curr, struct io_wait_queue, wq);; struct io_wait_queue *iowq = container_of(timer, struct io_wait_queue, t);; struct io_wait_queue *iowq = container_of(timer, struct io_wait_queue, t); |
| io_provide_buf | kbuf.c | addr, len, bgid, nbufs, bid | scoped_guard, for | list_for_each_entry, skb_walk_frags, switch, io_for_each_link, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, for, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, hlist_for_each_entry, list_for_each_entry_rcu | struct io_provide_buf {; struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);; struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf); |
| io_buffer_list | kbuf.h | buf_list | scoped_guard, for | list_for_each_entry, skb_walk_frags, switch, io_for_each_link, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, for, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, hlist_for_each_entry, list_for_each_entry_rcu | static bool io_kbuf_inc_commit(struct io_buffer_list *bl, int len); struct io_buffer_list *bl, int len, int nr); static inline struct io_buffer_list *io_buffer_get_list(struct io_ring_ctx *ctx, |
| io_buffer | kbuf.h | list, addr, len, bid, bgid | switch, scoped_guard, for | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | static bool io_kbuf_inc_commit(struct io_buffer_list *bl, int len); struct io_buffer_list *bl, int len, int nr); static inline struct io_buffer_list *io_buffer_get_list(struct io_ring_ctx *ctx, |
| buf_sel_arg | kbuf.h | out_len, max_len, nr_iovs, mode | - | - | static int io_ring_buffers_peek(struct io_kiocb *req, struct buf_sel_arg *arg,; int io_buffers_select(struct io_kiocb *req, struct buf_sel_arg *arg,; int io_buffers_peek(struct io_kiocb *req, struct buf_sel_arg *arg) |
| io_msg | msg_ring.c | tw, user_data, len, cmd, src_fd, dst_fd, cqe_flags | switch, while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_msg {; void io_msg_ring_cleanup(struct io_kiocb *req); struct io_msg *msg = io_kiocb_to_cmd(req, struct io_msg); |
| io_napi_entry | napi.c | napi_id, list, timeout, node, rcu | hlist_for_each_entry_rcu, list_for_each_entry_rcu | list_for_each_entry_rcu, list_for_each_entry, switch, scoped_guard, if, while, hlist_for_each_entry_rcu, wq_list_for_each, hlist_nulls_for_each_entry_rcu, for | struct io_napi_entry {; static struct io_napi_entry *io_napi_hash_find(struct hlist_head *hash_list,; struct io_napi_entry *e; |
| io_shutdown | net.c | how | __wq_list_for_each, for | list_for_each_entry, skb_walk_frags, switch, io_for_each_link, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, if, xa_for_each, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, hlist_for_each_entry, list_for_each_entry_rcu | io_shutdown_zcrx_ifqs(ctx);; struct io_shutdown {; int io_shutdown_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe) |
| io_accept | net.c | flags, iou_flags, file_slot, nofile | while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_accept {; int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_accept *accept = io_kiocb_to_cmd(req, struct io_accept); |
| io_socket | net.c | domain, type, protocol, flags, file_slot, nofile | while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_socket {; int io_socket_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_socket *sock = io_kiocb_to_cmd(req, struct io_socket); |
| io_connect | net.c | addr_len, in_progress, seen_econnaborted | while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_connect {; int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_connect *conn = io_kiocb_to_cmd(req, struct io_connect); |
| io_bind | net.c | addr_len | while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_bind {; int io_bind_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_bind *bind = io_kiocb_to_cmd(req, struct io_bind); |
| io_listen | net.c | backlog | while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_listen {; int io_listen_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_listen *listen = io_kiocb_to_cmd(req, struct io_listen); |
| io_sr_msg | net.c |  | while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_sr_msg {; struct io_sr_msg *sr = io_kiocb_to_cmd(req, struct io_sr_msg);; struct io_sr_msg *sr = io_kiocb_to_cmd(req, struct io_sr_msg); |
| io_recvzc | net.c | msg_flags, flags, len | - | - | struct io_recvzc {; int io_recvzc_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_recvzc *zc = io_kiocb_to_cmd(req, struct io_recvzc); |
| io_recvmsg_multishot_hdr | net.c | msg, addr | - | - | struct io_recvmsg_multishot_hdr {; struct io_recvmsg_multishot_hdr hdr;; BUILD_BUG_ON(offsetof(struct io_recvmsg_multishot_hdr, addr) != |
| io_async_msghdr | net.h | vec, namelen, fast_iov, controllen, payloadlen, msg, addr | io_for_each_link, while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | sizeof(struct io_async_msghdr),; offsetof(struct io_async_msghdr, clear));; static void io_netmsg_iovec_free(struct io_async_msghdr *kmsg) |
| io_nop | nop.c | result, fd, flags | - | - | struct io_nop {; int io_nop_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe); struct io_nop *nop = io_kiocb_to_cmd(req, struct io_nop); |
| io_notif_data | notif.h | uarg, account_pages, zc_report, zc_used, zc_copied | if | list_for_each_entry, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_notif_data *nd = io_notif_to_data(notif);; struct io_notif_data *nd = io_notif_to_data(notif);; struct io_notif_data *nd = container_of(uarg, struct io_notif_data, uarg); |
| io_issue_def | opdef.h | async_size | io_for_each_link, __wq_list_for_each | list_for_each_entry, switch, io_for_each_link, list_for_each_prev, __wq_list_for_each, list_for_each_entry_safe, if, xa_for_each, while, list_for_each_entry_reverse | const struct io_issue_def *def = &io_issue_defs[req->opcode];; static bool io_assign_file(struct io_kiocb *req, const struct io_issue_def *def,; const struct io_issue_def *def) |
| io_cold_def | opdef.h |  | io_for_each_link | list_for_each_entry, switch, list_for_each_prev, __wq_list_for_each, list_for_each_entry_safe, if, xa_for_each, while, list_for_each_entry_reverse | const struct io_cold_def *def = &io_cold_defs[req->opcode];; const struct io_cold_def *def = &io_cold_defs[req->opcode];; const struct io_cold_def io_cold_defs[] = { |
| io_open | openclose.c | dfd, file_slot, how, nofile | - | - | .prep			= io_openat_prep,; .issue			= io_openat,; .prep			= io_openat2_prep, |
| io_close | openclose.c | fd, file_slot | if, for | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | .prep			= io_close_prep,; .issue			= io_close,; struct io_close { |
| io_fixed_install | openclose.c | o_flags | if | list_for_each_entry, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_fixed_install {; struct io_fixed_install *ifi;; ifi = io_kiocb_to_cmd(req, struct io_fixed_install); |
| io_poll_update | poll.c | old_user_data, new_user_data, events, update_events, update_user_data | hlist_for_each_entry | list_for_each_entry, switch, hlist_for_each_entry_safe, if, for | struct io_poll_update {; struct io_poll_update *upd = io_kiocb_to_cmd(req, struct io_poll_update);; struct io_poll_update *poll_update = io_kiocb_to_cmd(req, struct io_poll_update); |
| io_poll_table | poll.c | pt, nr_entries, error, owning, result_mask | hlist_for_each_entry | list_for_each_entry, switch, hlist_for_each_entry_safe, if, for | struct io_poll_table {; static void __io_queue_proc(struct io_poll *poll, struct io_poll_table *pt,; struct io_poll_table *pt = container_of(p, struct io_poll_table, pt); |
| io_poll | poll.h | events, retries, wait | list_for_each_entry, switch, io_for_each_link, __wq_list_for_each, hlist_for_each_entry_safe, while, hlist_for_each_entry | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | ret = io_poll_cancel(ctx, cd, issue_flags);; io_poll_wq_wake(ctx);; io_poll_task_func, io_req_rw_complete, |
| async_poll | poll.h | poll | io_for_each_link, while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | sizeof(struct async_poll), 0);; struct async_poll *apoll = req->apoll;; struct async_poll *apoll = pt->req->apoll; |
| io_ring_ctx_rings | register.c | sq_region, ring_region | list_for_each_entry | list_for_each_entry_rcu, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_ring_ctx_rings {; struct io_ring_ctx_rings *r); struct io_ring_ctx_rings o = { }, n = { }, *to_free = NULL; |
| io_rsrc_update | rsrc.c | arg, nr_args, offset | - | - | struct io_rsrc_update {; struct io_rsrc_update *up = io_kiocb_to_cmd(req, struct io_rsrc_update);; struct io_rsrc_update *up = io_kiocb_to_cmd(req, struct io_rsrc_update); |
| io_rsrc_node | rsrc.h | type, refs, tag, file_ptr | switch, if, list_for_each_entry, __wq_list_for_each | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_rsrc_node *node;; node = io_rsrc_node_lookup(&ctx->file_table.data, fd);; struct io_rsrc_node *node; |
| io_mapped_ubuf | rsrc.h | ubuf, len, nr_bvecs, folio_shift, refs, acct_pages, is_kbuf, dir | switch, if | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_mapped_ubuf *buf = NULL;; struct io_mapped_ubuf *imu = priv;; static struct io_mapped_ubuf *io_alloc_imu(struct io_ring_ctx *ctx, |
| io_imu_folio_data | rsrc.h | nr_pages_head, nr_pages_mid, folio_shift, nr_folios | switch | list_for_each_entry_rcu, list_for_each_entry, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_imu_folio_data ifd;; struct io_imu_folio_data *data); struct io_imu_folio_data *data) |
| io_rw | rw.c | kiocb, addr, len, flags | switch, io_for_each_link, wq_list_for_each_resume | list_for_each_entry_rcu, list_for_each_entry, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | io_alloc_cache_free(&ctx->rw_cache, io_rw_cache_free);; .fail			= io_rw_fail,; .fail			= io_rw_fail, |
| io_meta_state | rw.h | seed, iter_meta | - | - | struct io_meta_state {; struct io_meta_state		meta_state; |
| io_async_rw | rw.h | vec, bytes_done, iter, iter_state, fast_iov, wpq, meta, meta_state | switch, io_for_each_link, wq_list_for_each_resume | list_for_each_entry_rcu, list_for_each_entry, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | sizeof(struct io_async_rw),; offsetof(struct io_async_rw, clear));; .async_size		= sizeof(struct io_async_rw), |
| io_splice | splice.c | off_out, off_in, len, splice_fd_in, flags | - | - | .prep			= io_splice_prep,; .issue			= io_splice,; .cleanup		= io_splice_cleanup, |
| io_sq_data | sqpoll.h | refs, park_pending, lock, ctx_list, wait, sq_thread_idle, sq_cpu, task_pid, task_tgid, work_time, state, exited | switch, list_for_each_entry, __wq_list_for_each | list_for_each_entry_rcu, list_for_each_entry, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_sq_data *sq = ctx->sq_data;; struct io_sq_data *sqd = ctx->sq_data;; __cold void io_uring_cancel_generic(bool cancel_all, struct io_sq_data *sqd) |
| io_statx | statx.c | dfd, mask, flags | - | - | .prep			= io_statx_prep,; .issue			= io_statx,; .cleanup		= io_statx_cleanup, |
| io_sync | sync.c | len, off, flags, mode | list_for_each_entry | list_for_each_entry_rcu, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, list_for_each_entry_reverse, for | static int __io_sync_cancel(struct io_uring_task *tctx,; int io_sync_cancel(struct io_ring_ctx *ctx, void __user *arg); ret = __io_sync_cancel(current->io_uring, &cd, sc.fd); |
| io_tctx_node | tctx.h | ctx_node | switch, list_for_each_entry_reverse, list_for_each_entry, __wq_list_for_each | list_for_each_entry_rcu, list_for_each_entry, switch, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | struct io_tctx_node *node;; struct io_tctx_node *node;; node = list_first_entry(&ctx->tctx_list, struct io_tctx_node, |
| io_timeout | timeout.c | off, target_seq, repeats, list | switch, list_for_each_prev, while, io_for_each_link | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | ret = io_timeout_cancel(ctx, cd);; .async_size		= sizeof(struct io_timeout_data),; .prep			= io_timeout_prep, |
| io_timeout_rem | timeout.c | addr, ts, flags, ltimeout | switch | list_for_each_entry_rcu, list_for_each_entry, io_for_each_link, list_for_each_prev, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, list_for_each_entry_safe, if, xa_for_each, while, hlist_for_each_entry_rcu, wq_list_for_each_resume, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, for | .prep			= io_timeout_remove_prep,; .issue			= io_timeout_remove,; struct io_timeout_rem { |
| io_timeout_data | timeout.h | timer, ts, mode, flags | switch, list_for_each_prev, while | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | .async_size		= sizeof(struct io_timeout_data),; .async_size		= sizeof(struct io_timeout_data),; struct io_timeout_data *data = req->async_data; |
| io_ftrunc | truncate.c | len | - | - | .prep			= io_ftruncate_prep,; .issue			= io_ftruncate,; struct io_ftrunc { |
| io_async_cmd | uring_cmd.h | data, vec | if, io_for_each_link | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | sizeof(struct io_async_cmd),; sizeof(struct io_async_cmd));; .async_size		= sizeof(struct io_async_cmd), |
| io_waitid | waitid.c | which, upid, options, refs, info | switch, if, list_for_each_entry | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | ret = io_waitid_cancel(ctx, cd, issue_flags);; ret |= io_waitid_remove_all(ctx, tctx, cancel_all);; .async_size		= sizeof(struct io_waitid_async), |
| io_waitid_async | waitid.h | wo | - | - | .async_size		= sizeof(struct io_waitid_async),; struct io_waitid_async *iwa = req->async_data;; struct io_waitid_async *iwa = req->async_data; |
| io_xattr | xattr.c | ctx | if | list_for_each_entry, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | .cleanup		= io_xattr_cleanup,; .cleanup		= io_xattr_cleanup,; .cleanup		= io_xattr_cleanup, |
| io_zcrx_args | zcrx.c | nr_skbs | while, skb_walk_frags | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_zcrx_args {; struct io_zcrx_args *args = desc->arg.data;; struct io_zcrx_args args = { |
| io_zcrx_area | zcrx.h | nia, is_mapped, area_id, ____cacheline_aligned_in_smp, free_count | for | list_for_each_entry, skb_walk_frags, switch, io_for_each_link, scoped_guard, __wq_list_for_each, hlist_for_each_entry_safe, if, xa_for_each, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, list_for_each_entry_reverse, hlist_for_each_entry, list_for_each_entry_rcu | struct io_zcrx_area *area, int nr_mapped); static void io_zcrx_unmap_area(struct io_zcrx_ifq *ifq, struct io_zcrx_area *area); static int io_zcrx_map_area(struct io_zcrx_ifq *ifq, struct io_zcrx_area *area) |
| io_zcrx_ifq | zcrx.h | rq_entries, cached_rq_head, rq_lock, if_rxq, netdev_tracker, lock | skb_walk_frags, while, for | list_for_each_entry, if, list_for_each_entry_reverse, for, skb_walk_frags, io_for_each_link, hlist_for_each_entry_safe, hlist_for_each_entry_rcu, list_for_each_entry_rcu, switch, list_for_each_prev, scoped_guard, list_for_each_entry_safe, while, wq_list_for_each, hlist_nulls_for_each_entry_rcu, hlist_for_each_entry, __wq_list_for_each, wq_list_for_each_resume, xa_for_each | struct io_zcrx_ifq		*ifq;; static void __io_zcrx_unmap_area(struct io_zcrx_ifq *ifq,; static void io_zcrx_unmap_area(struct io_zcrx_ifq *ifq, struct io_zcrx_area *area) |
